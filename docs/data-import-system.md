# Data Import System - XML & Web Scraping

**Complete documentation for the Unified Data Import Hub**

**Status:** âœ… Phase 1 & 2 Complete (XML Import with Dynamic Mapping & Backend Processing)

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Phase 1: Frontend & Edge Function](#phase-1-frontend--edge-function)
4. [Phase 2: Backend Data Processing](#phase-2-backend-data-processing)
5. [API Reference](#api-reference)
6. [Database Schema](#database-schema)
7. [Usage Guide](#usage-guide)
8. [Testing](#testing)
9. [Performance](#performance)
10. [Future Phases](#future-phases)

---

## Overview

The Data Import System enables ingesting products from multiple sources (XML files, web scraping via Firecrawl, and existing PDF processing) through a unified data import hub. It provides dynamic field mapping, AI-assisted configuration, batch processing, and real-time progress tracking.

### Key Features

**Phase 1 (Complete):**
- âœ… Dynamic XML field mapping with AI suggestions (Claude Sonnet 4.5)
- âœ… Reusable mapping templates
- âœ… Manual re-run functionality
- âœ… Cron-based scheduling for recurring imports
- âœ… Real-time progress tracking
- âœ… Import history with audit trail

**Phase 2 (Complete):**
- âœ… Backend batch processing (10 products at a time)
- âœ… Concurrent image downloads (5 parallel)
- âœ… Direct product creation without PDF pipeline
- âœ… Image linking to products
- âœ… Async text processing (chunking, embeddings)
- âœ… Checkpoint recovery for failed jobs
- âœ… Comprehensive error handling

**Phase 3 (Planned):**
- Frontend real-time progress tracking
- Mapping template management UI
- Import job monitoring dashboard

**Phase 4 (Planned):**
- Firecrawl web scraping integration
- Same dynamic mapping approach as XML

### Use Cases

1. **Supplier Catalog Imports** - Import products from supplier XML catalogs
2. **Recurring Updates** - Schedule automatic imports from supplier URLs
3. **Manual Re-runs** - Re-import catalogs with one click
4. **Multi-source Integration** - Combine XML, web scraping, and PDF sources

---

## Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND (DataImportHub)                                    â”‚
â”‚ â”œâ”€ XML Import Tab                                           â”‚
â”‚ â”œâ”€ Web Scraping Tab (Phase 4)                               â”‚
â”‚ â””â”€ Import History Tab                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EDGE FUNCTION (xml-import-orchestrator)                     â”‚
â”‚ â”œâ”€ Parse XML and detect fields                              â”‚
â”‚ â”œâ”€ AI-powered field mapping (Claude Sonnet 4.5)             â”‚
â”‚ â”œâ”€ Create data_import_jobs record                           â”‚
â”‚ â””â”€ Call Python API (non-blocking)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PYTHON API (DataImportService)                              â”‚
â”‚ â”œâ”€ Batch processing (10 products at a time)                 â”‚
â”‚ â”œâ”€ Image downloads (5 concurrent)                           â”‚
â”‚ â”œâ”€ Product creation with metadata                           â”‚
â”‚ â”œâ”€ Image linking via document_images                        â”‚
â”‚ â”œâ”€ Async text processing queue                              â”‚
â”‚ â””â”€ Real-time progress updates                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ASYNC PROCESSING (Background)                               â”‚
â”‚ â”œâ”€ Chunking (UnifiedChunkingService)                        â”‚
â”‚ â”œâ”€ Text Embeddings (RealEmbeddingsService)                  â”‚
â”‚ â””â”€ Product enrichment (optional)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
1. User uploads XML file
   â†“
2. Edge Function parses XML and detects fields
   â†“
3. AI suggests field mappings (Claude Sonnet 4.5)
   â†“
4. User reviews and confirms mappings
   â†“
5. Edge Function creates import job
   â†“
6. Python API processes job in batches
   â†“
7. Images downloaded concurrently
   â†“
8. Products created in database
   â†“
9. Images linked to products
   â†“
10. Text processing queued (async)
    â†“
11. Job marked as completed
```

---

## Phase 1: Frontend & Edge Function

### Components

#### 1. DataImportHub (`src/components/Admin/DataImportHub.tsx`)

Main hub component with 3 tabs:
- **XML Import Tab** - Upload and configure XML imports
- **Web Scraping Tab** - Firecrawl integration (Phase 4)
- **Import History Tab** - View past imports with re-run and scheduling

#### 2. XMLFieldMappingModal (`src/components/Admin/DataImport/XMLFieldMappingModal.tsx`)

Interactive UI for reviewing AI-suggested field mappings:
- Color-coded confidence badges (green â‰¥90%, yellow â‰¥70%, red <70%)
- Dropdown selectors for target schema fields
- Template saving functionality
- Preview of sample values

#### 3. ImportHistoryTab (`src/components/Admin/DataImport/ImportHistoryTab.tsx`)

Displays past import jobs with:
- Status and progress indicators
- Manual re-run button (blue, Play icon)
- Schedule cron button (purple, Calendar icon)
- Next run time for scheduled imports

#### 4. ScheduleImportModal (`src/components/Admin/DataImport/ScheduleImportModal.tsx`)

Configure cron schedules for recurring imports:
- 6 preset schedules (hourly, daily, weekly, etc.)
- Custom cron expression support
- Source URL input for scheduled imports

### Edge Functions

#### xml-import-orchestrator (`supabase/functions/xml-import-orchestrator/index.ts`)

**Purpose:** Parse XML, detect fields, suggest mappings, create import jobs

**Endpoints:**
- `POST /xml-import-orchestrator` - Upload XML and create import job

**Features:**
- XML parsing with field detection
- AI-powered field mapping using Claude Sonnet 4.5
- Fallback rule-based mapping (multi-language support)
- Preview mode for field detection only
- Stores products in job metadata for Python API

**Request:**
```typescript
{
  workspace_id: string;
  category: string;
  xml_content: string; // Base64 encoded
  preview_only?: boolean;
  field_mappings?: Record<string, string>;
  mapping_template_id?: string;
  parent_job_id?: string;
}
```

**Response (Preview Mode):**
```typescript
{
  success: true;
  detected_fields: DetectedField[];
  total_products: number;
}
```

**Response (Import Mode):**
```typescript
{
  success: true;
  job_id: string;
  total_products: number;
}
```

#### scheduled-import-runner (`supabase/functions/scheduled-import-runner/index.ts`)

**Purpose:** Run scheduled imports via Supabase Cron

**Trigger:** Supabase Cron (every 15 minutes)

**Features:**
- Fetches XML from source URLs
- Creates new import jobs with same field mappings
- Updates next_run_at timestamps
- Links to parent job via parent_job_id

---

## Phase 2: Backend Data Processing

### Services

#### 1. DataImportService (`mivaa-pdf-extractor/app/services/data_import_service.py`)

Main orchestrator for processing import jobs.

**Key Methods:**
- `process_import_job()` - Process complete import job
- `_process_batch()` - Process batch of 10 products
- `_normalize_product()` - Apply field mappings
- `_download_images()` - Download images concurrently
- `_queue_product_processing()` - Create products in database
- `_link_images_to_product()` - Link images to products
- `_queue_text_processing()` - Queue async text processing

**Features:**
- Batch processing (10 products at a time)
- Checkpoint recovery
- Real-time progress updates
- Error handling with detailed logging

#### 2. ImageDownloadService (`mivaa-pdf-extractor/app/services/image_download_service.py`)

Handles concurrent image downloads with validation and retry logic.

**Key Methods:**
- `download_images()` - Download multiple images concurrently
- `_download_single_image()` - Download single image with retry
- `validate_image_url()` - Validate URL format
- `store_image_in_storage()` - Upload to Supabase Storage

**Features:**
- Concurrent downloads (5 parallel)
- URL validation
- Content validation (type, size)
- Retry logic (3 attempts with exponential backoff)
- 10MB file size limit
- 30s timeout per download

### API Routes

#### Data Import Routes (`mivaa-pdf-extractor/app/api/data_import_routes.py`)

**Endpoints:**

1. **POST /api/import/process** - Start processing an import job
2. **GET /api/import/jobs/{job_id}** - Get import job status
3. **GET /api/import/history** - Get import history
4. **GET /api/import/health** - Health check

See [API Reference](#api-reference) for detailed documentation.

---

## API Reference

### Edge Function API

#### POST /xml-import-orchestrator

Upload XML file and create import job.

**Request Body:**
```json
{
  "workspace_id": "uuid",
  "category": "materials",
  "xml_content": "base64_encoded_xml",
  "preview_only": false,
  "field_mappings": {
    "name": "name",
    "factory": "factory_name",
    "category": "material_category"
  },
  "mapping_template_id": "uuid",
  "parent_job_id": "uuid"
}
```

**Response:**
```json
{
  "success": true,
  "job_id": "uuid",
  "total_products": 10
}
```

### Python API

#### POST /api/import/process

Start processing an import job (called by Edge Function).

**Request Body:**
```json
{
  "job_id": "uuid",
  "workspace_id": "uuid"
}
```

**Response:**
```json
{
  "success": true,
  "message": "Import job processing started",
  "job_id": "uuid"
}
```

#### GET /api/import/jobs/{job_id}

Get import job status and progress.

**Response:**
```json
{
  "job_id": "uuid",
  "status": "processing",
  "import_type": "xml",
  "source_name": "Supplier Catalog",
  "total_products": 100,
  "processed_products": 45,
  "failed_products": 2,
  "progress_percentage": 45,
  "current_stage": "downloading_images",
  "started_at": "2025-11-10T10:00:00Z",
  "completed_at": null,
  "error_message": null,
  "estimated_time_remaining": 120
}
```

#### GET /api/import/history

Get import history for a workspace.

**Query Parameters:**
- `workspace_id` (required) - Workspace ID
- `page` (optional, default: 1) - Page number
- `page_size` (optional, default: 20) - Items per page
- `status` (optional) - Filter by status
- `import_type` (optional) - Filter by import type

**Response:**
```json
{
  "imports": [
    {
      "job_id": "uuid",
      "import_type": "xml",
      "source_name": "Supplier Catalog",
      "status": "completed",
      "total_products": 100,
      "processed_products": 98,
      "failed_products": 2,
      "created_at": "2025-11-10T10:00:00Z",
      "completed_at": "2025-11-10T10:15:00Z"
    }
  ],
  "total_count": 50,
  "page": 1,
  "page_size": 20
}
```

#### GET /api/import/health

Health check for data import API.

**Response:**
```json
{
  "status": "healthy",
  "service": "data-import-api",
  "version": "1.0.0",
  "features": {
    "xml_import": true,
    "web_scraping": false,
    "batch_processing": true,
    "concurrent_image_downloads": true,
    "checkpoint_recovery": true,
    "real_time_progress": true
  }
}
```

---

## Database Schema

### data_import_jobs

Tracks import jobs with status and progress.

```sql
CREATE TABLE data_import_jobs (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  workspace_id UUID NOT NULL REFERENCES workspaces(id),
  import_type TEXT NOT NULL, -- 'xml', 'web_scraping'
  source_name TEXT,
  source_url TEXT,
  status TEXT NOT NULL DEFAULT 'pending', -- 'pending', 'processing', 'completed', 'failed'
  total_products INTEGER DEFAULT 0,
  processed_products INTEGER DEFAULT 0,
  failed_products INTEGER DEFAULT 0,
  category TEXT,
  original_xml_content TEXT, -- For re-runs
  field_mappings JSONB, -- User-configured field mappings
  mapping_template_id UUID REFERENCES xml_mapping_templates(id),
  parent_job_id UUID REFERENCES data_import_jobs(id), -- For re-runs and scheduled runs
  is_scheduled BOOLEAN DEFAULT FALSE,
  cron_schedule TEXT,
  last_run_at TIMESTAMPTZ,
  next_run_at TIMESTAMPTZ,
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  error_message TEXT,
  metadata JSONB, -- Stores products for processing
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

### data_import_history

Tracks individual product imports for audit trail.

```sql
CREATE TABLE data_import_history (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  job_id UUID NOT NULL REFERENCES data_import_jobs(id) ON DELETE CASCADE,
  source_data JSONB NOT NULL, -- Original product data from XML
  normalized_data JSONB, -- Normalized product data after field mapping
  processing_status TEXT NOT NULL DEFAULT 'pending', -- 'pending', 'success', 'failed'
  error_details TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

### xml_mapping_templates

Stores reusable field mapping templates.

```sql
CREATE TABLE xml_mapping_templates (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  workspace_id UUID NOT NULL REFERENCES workspaces(id),
  name TEXT NOT NULL,
  description TEXT,
  field_mappings JSONB NOT NULL, -- XML field -> Platform field mappings
  created_by UUID REFERENCES auth.users(id),
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(workspace_id, name)
);
```

---

## Usage Guide

### 1. Upload XML File

1. Navigate to Admin Dashboard â†’ Data Import Hub
2. Click "XML Import" tab
3. Select category (e.g., "materials")
4. Upload XML file
5. Review AI-suggested field mappings
6. Adjust mappings if needed
7. Optionally save as template
8. Click "Import"

### 2. Schedule Recurring Import

1. Go to Import History tab
2. Find completed import
3. Click "Schedule Cron" button
4. Enter source URL
5. Select schedule (hourly, daily, weekly, custom)
6. Click "Schedule"

### 3. Manual Re-run

1. Go to Import History tab
2. Find completed import
3. Click "Manual Re-run" button
4. Confirm re-run
5. New job created with same mappings

---

## Testing

### Integration Test Script

**Location:** `scripts/testing/test-xml-import-phase2.js`

**Usage:**
```bash
node scripts/testing/test-xml-import-phase2.js
```

**Test Flow:**
1. Upload XML with 3 sample products
2. Monitor job progress (polls every 5s, max 5 min)
3. Verify products created in database
4. Verify images downloaded and linked
5. Verify import history records
6. Display comprehensive summary

---

## Performance

### Batch Processing
- **Batch Size:** 10 products
- **Concurrent Images:** 5 per batch
- **Checkpoint Frequency:** After each batch
- **Memory Management:** Garbage collection after each batch

### Image Downloads
- **Max File Size:** 10MB
- **Timeout:** 30 seconds per image
- **Retry Attempts:** 3 with exponential backoff
- **Storage:** Supabase `pdf-tiles` bucket

### Database Operations
- **Products:** Direct insert to `products` table
- **Images:** Linked via `document_images` table
- **Chunks:** Created in `chunks` table
- **History:** Tracked in `data_import_history` table

---

## Future Phases

### Phase 3: Frontend Updates
- Real-time progress tracking in UI
- Mapping template management
- Import job monitoring dashboard

### Phase 4: Web Scraping Integration
- Firecrawl integration
- Same dynamic mapping approach as XML
- Product extraction from websites
- Flows through same `data_import_jobs` pipeline

